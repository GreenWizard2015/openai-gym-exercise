# DDPG

Это почти копия [примера с сайта keras.io](https://keras.io/examples/rl/ddpg_pendulum/). Отличия:

- Код организован в более объектно-ориентированной манере. DDPG я планирую использовать в другом проекте, поэтому сразу решил реализовать удобную обёртку.

- Процесс обучения происходит по окончанию тестирования, а не на каждом шагу. Это не особо влияет на качество обучения, но существенно упрощает код. Впрочем, если мы имеем дело со сложной средой, то лучше дообучать агента на каждом шагу, чтоб он мог адаптироваться к новым условиям и, главное, иметь шанс собрать больше данных.

- Уровень шума постепенно снижается, чтоб агент мог найти более оптимальные стратегии.

Пока писал данную реализацию, я усвоил следующие "уроки":

- Должен быть работающий пример реализации, чтоб сверятся с ним. Я узнал о наличие множества мелких нюансов только благодаря сравнению результатов своего кода с примером.

- Не стоит слепо нормализировать данные. Хоть в ML это считается основой основ, но лучше иметь сведенья о работе на "сырых" данных. Так, например, нормализация текущей скорости приводила к снижению качества обучения. Вычитание старой награды из текущей так же снижает стабильность агента. Предполагаю, что важную роль имеют и сами награды, а не только их разности, поэтому (именно в данном случае) лучше их передавать в исходном виде.
  
- Крайне важно проверять, как минимум, формат приходящих в loss данных т. к. очень просто получить вполне работающий код, но делающий абсолютно не то что ожидаешь. Простая строка `tf.assert_equal(tf.shape(y_true), tf.shape(y_pred))` может избавить от сюрпризов из-за неявного преобразования данных.

- Стоит тщательно протестировать оригинальное решение, прежде чем вносить свои правки и делать какие-то выводы. Постепенное изменение весов target-сети мне всегда казалось, как минимум, излишним, поэтому я везде использую hard update, но тут я впервые увидел насколько важным может быть soft update.